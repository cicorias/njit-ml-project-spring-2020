{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "common_seed = 42\n",
    "np.random.seed(common_seed)\n",
    "tf.random.set_seed(common_seed)\n",
    "random.seed(common_seed)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class TrainParameters():\n",
    "    import os\n",
    "    import time\n",
    "    from typing import List\n",
    "    from dataclasses import field\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.python.keras.engine import base_layer\n",
    "    \n",
    "    window_size : int = 60\n",
    "    predict_step : int = 1\n",
    "    test_split  : float = 0.20\n",
    "    feature_columns : list = field(default_factory = lambda: [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"])\n",
    "    scale : bool = True\n",
    "    shuffle : bool = True\n",
    "    date_now : time = time.strftime(\"%Y-%m-%d\")\n",
    "    model_layers: int = 3\n",
    "    model_cell : base_layer = LSTM\n",
    "    lstm_neurons : int = 256\n",
    "    dropout : float = 0.40\n",
    "    \n",
    "    ### training parameters\n",
    "    # mean absolute error loss\n",
    "    # LOSS = \"mae\"\n",
    "    # huber loss\n",
    "    loss : str = \"huber_loss\"\n",
    "    optimizer: str = \"adam\"\n",
    "    batch_size : int = 64\n",
    "    epochs : int = 400\n",
    "\n",
    "    ticker : str = 'MSFT'\n",
    "\n",
    "    #both must be dynamoic as they are resolved late.\n",
    "    @property\n",
    "    def ticker_file(self):\n",
    "        import os\n",
    "        return os.path.join(\"data\", f\"{self.ticker}_{self.date_now}.csv\")\n",
    "        \n",
    "    @property\n",
    "    def model_name(self):\n",
    "        # model name to save, making it as unique as possible based on parameters\n",
    "        return f\"{self.date_now}_{self.ticker}-{self.loss}-{self.optimizer}-{self.model_cell.__name__}-seq-{self.window_size}-step-{self.predict_step}-layers-{self.model_layers}-units-{self.lstm_neurons}\"\n",
    "\n",
    "\n",
    "def load_data(p : TrainParameters):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        window_size (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the data, default is True\n",
    "        predict_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from yahoo_fin import stock_info as si\n",
    "    from collections import deque\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(p.ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(p.ticker)\n",
    "    elif isinstance(p.ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = p.ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in p.feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    if p.scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in p.feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `predict_step`\n",
    "    df['future'] = df['adjclose'].shift(-p.predict_step)\n",
    "\n",
    "    # last `predict_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[p.feature_columns].tail(p.predict_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=p.window_size)\n",
    "\n",
    "    for entry, target in zip(df[p.feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == p.window_size:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `predict_step` sequence\n",
    "    # for instance, if window_size=50 and predict_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    \n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=p.test_split, shuffle=p.shuffle)\n",
    "    # return the result\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_model(p : TrainParameters):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout    \n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(p.model_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            model.add(p.model_cell(p.lstm_neurons, return_sequences = True, input_shape = (None, p.window_size)))\n",
    "        elif i == p.model_layers - 1:\n",
    "            # last layer\n",
    "            model.add(p.model_cell(p.lstm_neurons, return_sequences= False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            model.add(p.model_cell(p.lstm_neurons, return_sequences = True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(p.dropout))\n",
    "        \n",
    "    #final layer is 1 output given \"prediction\"\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss = p.loss, metrics=[\"mean_absolute_error\"], optimizer = p.optimizer)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TrainParameters(ticker='MSFT', epochs=5)\n",
    "data = load_data(a)\n",
    "\n",
    "model = create_model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6836 samples, validate on 1710 samples\n",
      "Epoch 1/5\n",
      "6836/6836 [==============================] - 6s 942us/sample - loss: 9.0628e-04 - mean_absolute_error: 0.0246 - val_loss: 1.7659e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 2/5\n",
      "6836/6836 [==============================] - 3s 505us/sample - loss: 4.0502e-04 - mean_absolute_error: 0.0166 - val_loss: 1.5050e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 3/5\n",
      " 832/6836 [==>...........................] - ETA: 2s - loss: 2.8324e-04 - mean_absolute_error: 0.0138"
     ]
    }
   ],
   "source": [
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size = a.batch_size,\n",
    "                    epochs = a.epochs,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    #callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
