{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From\n",
    "\n",
    "https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras\n",
    "\n",
    "other idea\n",
    "https://hackernoon.com/forecasting-market-movements-using-tensorflow-fb73e614cd06\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "#import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "def create_model(input_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            model.add(cell(units, return_sequences=True, input_shape=(None, input_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "import time\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "### training parameters\n",
    "# mean squared error loss\n",
    "LOSS = \"mse\"\n",
    "OPTIMIZER = \"rmsprop\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TODO: realistic\n",
    "EPOCHS = 10 ##300\n",
    "# Apple stock market\n",
    "ticker = \"AAPL\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "# create these folders if they does not exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7899 samples, validate on 1975 samples\n",
      "Epoch 1/10\n",
      "7899/7899 [==============================] - 9s 1ms/sample - loss: 0.0039 - mean_absolute_error: 0.0277 - val_loss: 3.6249e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 2/10\n",
      "7899/7899 [==============================] - 5s 641us/sample - loss: 0.0021 - mean_absolute_error: 0.0211 - val_loss: 0.0013 - val_mean_absolute_error: 0.0161\n",
      "Epoch 3/10\n",
      "7899/7899 [==============================] - 5s 639us/sample - loss: 0.0018 - mean_absolute_error: 0.0205 - val_loss: 5.7753e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 4/10\n",
      "7899/7899 [==============================] - 5s 645us/sample - loss: 0.0014 - mean_absolute_error: 0.0192 - val_loss: 3.6697e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 5/10\n",
      "7899/7899 [==============================] - 5s 641us/sample - loss: 0.0013 - mean_absolute_error: 0.0187 - val_loss: 7.0044e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 6/10\n",
      "7899/7899 [==============================] - 5s 621us/sample - loss: 0.0012 - mean_absolute_error: 0.0183 - val_loss: 0.0016 - val_mean_absolute_error: 0.0207\n",
      "Epoch 7/10\n",
      "7899/7899 [==============================] - 5s 612us/sample - loss: 0.0012 - mean_absolute_error: 0.0177 - val_loss: 0.0017 - val_mean_absolute_error: 0.0208\n",
      "Epoch 8/10\n",
      "7899/7899 [==============================] - 5s 602us/sample - loss: 9.7316e-04 - mean_absolute_error: 0.0169 - val_loss: 1.5583e-04 - val_mean_absolute_error: 0.0047\n",
      "Epoch 9/10\n",
      "7899/7899 [==============================] - 5s 600us/sample - loss: 9.4607e-04 - mean_absolute_error: 0.0166 - val_loss: 1.5437e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 10/10\n",
      "7899/7899 [==============================] - 5s 608us/sample - loss: 9.1380e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0012 - val_mean_absolute_error: 0.0183\n"
     ]
    }
   ],
   "source": [
    "## hacking\n",
    "\n",
    "#tf.summary.create_file_writer('logs/fit').as_default()\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "# some tensorflow callbacks\n",
    "#checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_best_only=True, verbose=1)\n",
    "#tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "# see https://www.tensorflow.org/tensorboard/get_started\n",
    "# some bug local i cannot unset 'logs/hparam_tuning'\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 28006), started 0:32:48 ago. (Use '!kill 28006' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f2ef6cf464ed7628\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f2ef6cf464ed7628\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model testing\n",
    "\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
    "\n",
    "# construct the model - do NOT need to redo this..\n",
    "###model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "###                     dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"])\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform(mae.reshape(1, -1))[0][0]\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, classification=False):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
